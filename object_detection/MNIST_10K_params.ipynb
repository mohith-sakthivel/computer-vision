{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn_alYjPxVaG"
   },
   "source": [
    "### MNIST Classifer model with  < 10K parameters\n",
    "\n",
    "In this notebook, a conv net is trained on the MNIST dataset. The model is loosely based on **LeNet** with some modern enhancements.\n",
    "\n",
    "The Model has **<10K parameters** and achieves **>99.4% accuracy** on the MNIST test set.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5nvqOgBDWa7"
   },
   "source": [
    "#### Key techniques used:\n",
    "*   Training convergence and stability improvements\n",
    "  *   Batch Normalization\n",
    "  *   Adam Optimizer\n",
    "\n",
    "*   Regularization techniques\n",
    "  *   Data Augumentation\n",
    "  *   Weight Decay\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yq2V5wfUH6b6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jSdV1lkeumb7"
   },
   "outputs": [],
   "source": [
    "def get_size(img_shape, filters):\n",
    "  \"\"\" Calculate the shape of conv net output \"\"\"\n",
    "  out_shape = img_shape[-2:]\n",
    "  for f in filters:\n",
    "    if f[0] == 'pool':\n",
    "      out_shape.div_(f[1])\n",
    "    elif f[0] == 'conv':\n",
    "      k, s, p = f[1:]\n",
    "      out_shape = torch.floor(((out_shape + 2*p - (k-1) - 1)/s)+1)\n",
    "  return out_shape.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iwt7ns7-oTVO"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "  def __init__(self, input_shape, out_dim, fc_layers=[100]):\n",
    "    \n",
    "    super(Model, self).__init__()\n",
    "\n",
    "    # details of the conv net layers\n",
    "    lyrs = [('conv', 5, 1, 0),\n",
    "            ('conv', 5, 1, 0),\n",
    "            ('pool', 2),\n",
    "            ('conv', 3, 1, 0),\n",
    "            ('conv', 3, 1, 0),\n",
    "            ('pool', 2),\n",
    "            ('conv', 3, 1, 0),]\n",
    "\n",
    "    # initialize the conv layers\n",
    "    self.conv_1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0, bias=True)\n",
    "    self.conv_2 = nn.Conv2d(6, 6, kernel_size=5, stride=1, padding=0, bias=False)\n",
    "    self.bn_1 = nn.BatchNorm2d(6, affine=True)\n",
    "    self.conv_3 = nn.Conv2d(6, 16, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "    self.conv_4 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "    self.bn_2 = nn.BatchNorm2d(16, affine=True)\n",
    "\n",
    "    self.conv_5 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "    self.bn_3 = nn.BatchNorm2d(16, affine=True)\n",
    "\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    t_out = get_size(torch.tensor(input_shape), lyrs)\n",
    "    # print(f'ConvNet output shape: {t_out[0]}x{t_out[1]}')\n",
    "    self._conv_out_dim = int(np.prod(t_out)*16)\n",
    "\n",
    "    # initialize the fully-connected layers\n",
    "    self._fc = []\n",
    "    inp = self._conv_out_dim\n",
    "\n",
    "    for lyr_dim in fc_layers:\n",
    "      self._fc.append(nn.Linear(inp, lyr_dim, bias=False))\n",
    "      self._fc.append(nn.BatchNorm1d(lyr_dim, affine=True))\n",
    "      self._fc.append(nn.ReLU())\n",
    "      inp = lyr_dim\n",
    "    self._fc.append(nn.Linear(inp, out_dim))\n",
    "    self._fc = nn.Sequential(*self._fc)\n",
    "\n",
    "    self.parameter_count()\n",
    "\n",
    "  def parameter_count(self):\n",
    "    \"\"\" Calculate the number of parameters in the model \"\"\"\n",
    "    num_params=0\n",
    "    for params in self.parameters():\n",
    "      num_params += params.numel()\n",
    "    print(f'Model has {num_params} parameters...')\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.conv_1(x))\n",
    "    x = self.pool(F.relu(self.bn_1(self.conv_2(x))))\n",
    "    x = F.relu(self.conv_3(x))\n",
    "    x = self.pool(F.relu(self.bn_2(self.conv_4(x))))\n",
    "    x = F.relu(self.bn_3(self.conv_5(x)))\n",
    "    x = x.view(-1, self._conv_out_dim)\n",
    "    x = self._fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M93Uvd4u6Fn3"
   },
   "outputs": [],
   "source": [
    "# major hyperparameters\n",
    "seed = 20\n",
    "lr = 0.002\n",
    "batch_size = 64\n",
    "num_epochs = 40\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RDMiUhcFnVhg"
   },
   "outputs": [],
   "source": [
    "# set seed\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DHnRpF7hh9t0"
   },
   "outputs": [],
   "source": [
    "# data preprocessing and augumentation\n",
    "da_transforms = transforms.RandomApply([transforms.RandomAffine(10, scale=(0.95, 1.05), translate=(0.1, 0.1))], p=0.3)\n",
    "\n",
    "train_transform = transforms.Compose([da_transforms,\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, ), (0.5, ))])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, ), (0.5, ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_QKJreT2j54D"
   },
   "outputs": [],
   "source": [
    "# setup the train/test data loaders\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True,\n",
    "                          download=True, transform=train_transform)\n",
    "testset = datasets.MNIST(root='./data', train=False,\n",
    "                         download=True, transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Bt16dSHqETL",
    "outputId": "040c3495-6f7f-4163-8060-f5ee27fe8153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 9430 parameters...\n",
      "Using cuda...\n"
     ]
    }
   ],
   "source": [
    "# setup model, loss, and optimizer\n",
    "\n",
    "network = Model(trainset.data.shape[-3:], len(trainset.classes))\n",
    "network.to(device)\n",
    "print(f'Using {device}...')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=lr, weight_decay=weight_decay, eps=1e-05)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KpgJ0LOu-KI-"
   },
   "outputs": [],
   "source": [
    "def train(net, criterion, optimizer, scheduler=None, num_epochs=30, train_set=trainloader, test_set=testloader):\n",
    "  \"\"\"\n",
    "  Trains the provided model for specified number of epochs\n",
    "  \"\"\"\n",
    "  loss_hist = []\n",
    "  time_hist = []\n",
    "\n",
    "  for epoch in range(1, num_epochs+1):\n",
    "    loss_hist.append([])\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, data in enumerate(train_set, 0):\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = net(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss_hist[-1].append(loss.cpu().item())\n",
    "    \n",
    "    loss_hist[-1] = np.round(np.mean(loss_hist[-1]), 4)\n",
    "    time_hist.append(np.round(time.time()-start_time, 2))\n",
    "    print(f'Epoch {epoch}  -  loss: {loss_hist[-1]}  time: {time_hist[-1]}')\n",
    "    if (epoch%5) == 0:\n",
    "      print('---------------------------------------')\n",
    "      get_accuracy(network, train_set, label='Train')\n",
    "      get_accuracy(network, test_set, label='Test')\n",
    "      print('---------------------------------------')\n",
    "      network.train()\n",
    "    if scheduler is not None:\n",
    "      scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PfV0KDdeF8lo"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataloader, label='Test'):\n",
    "  \"\"\" Test the performance of the network on the provided dataset \"\"\"\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    c = []\n",
    "    for data in dataloader:\n",
    "      images, labels = data[0].to(device), data[1].to(device)\n",
    "      outputs = model(images)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      c.append((predicted == labels).squeeze())\n",
    "\n",
    "    c = torch.cat(c, dim=0)\n",
    "    acc = 100*(c.sum()/len(c))\n",
    "    print(f'{label} Accuracy: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCKo5MpJL_m0",
    "outputId": "0688ec39-ab9e-4582-fa9b-f09429b92dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  -  loss: 0.2513  time: 17.11\n",
      "Epoch 2  -  loss: 0.1096  time: 16.87\n",
      "Epoch 3  -  loss: 0.0869  time: 16.87\n",
      "Epoch 4  -  loss: 0.077  time: 17.45\n",
      "Epoch 5  -  loss: 0.0771  time: 17.43\n",
      "---------------------------------------\n",
      "Train Accuracy: 97.81%\n",
      "Test Accuracy: 98.69%\n",
      "---------------------------------------\n",
      "Epoch 6  -  loss: 0.0742  time: 17.85\n",
      "Epoch 7  -  loss: 0.0724  time: 17.56\n",
      "Epoch 8  -  loss: 0.0699  time: 17.43\n",
      "Epoch 9  -  loss: 0.0662  time: 17.44\n",
      "Epoch 10  -  loss: 0.0653  time: 17.49\n",
      "---------------------------------------\n",
      "Train Accuracy: 98.51%\n",
      "Test Accuracy: 99.06%\n",
      "---------------------------------------\n",
      "Epoch 11  -  loss: 0.0472  time: 18.51\n",
      "Epoch 12  -  loss: 0.0414  time: 16.87\n",
      "Epoch 13  -  loss: 0.0401  time: 17.35\n",
      "Epoch 14  -  loss: 0.0395  time: 19.4\n",
      "Epoch 15  -  loss: 0.0383  time: 18.36\n",
      "---------------------------------------\n",
      "Train Accuracy: 99.20%\n",
      "Test Accuracy: 99.37%\n",
      "---------------------------------------\n",
      "Epoch 16  -  loss: 0.0386  time: 17.68\n",
      "Epoch 17  -  loss: 0.0403  time: 17.49\n",
      "Epoch 18  -  loss: 0.0377  time: 17.57\n",
      "Epoch 19  -  loss: 0.0371  time: 17.46\n",
      "Epoch 20  -  loss: 0.0376  time: 17.58\n",
      "---------------------------------------\n",
      "Train Accuracy: 99.25%\n",
      "Test Accuracy: 99.39%\n",
      "---------------------------------------\n",
      "Epoch 21  -  loss: 0.0329  time: 16.95\n",
      "Epoch 22  -  loss: 0.0306  time: 18.12\n",
      "Epoch 23  -  loss: 0.031  time: 18.28\n",
      "Epoch 24  -  loss: 0.0307  time: 17.11\n",
      "Epoch 25  -  loss: 0.0289  time: 18.22\n",
      "---------------------------------------\n",
      "Train Accuracy: 99.44%\n",
      "Test Accuracy: 99.45%\n",
      "---------------------------------------\n",
      "Epoch 26  -  loss: 0.0302  time: 17.13\n",
      "Epoch 27  -  loss: 0.03  time: 17.6\n",
      "Epoch 28  -  loss: 0.0299  time: 17.49\n",
      "Epoch 29  -  loss: 0.0298  time: 16.97\n",
      "Epoch 30  -  loss: 0.0291  time: 17.08\n",
      "---------------------------------------\n",
      "Train Accuracy: 99.51%\n",
      "Test Accuracy: 99.49%\n",
      "---------------------------------------\n",
      "Epoch 31  -  loss: 0.0277  time: 16.91\n",
      "Epoch 32  -  loss: 0.0284  time: 17.07\n",
      "Epoch 33  -  loss: 0.0293  time: 17.11\n",
      "Epoch 34  -  loss: 0.0294  time: 17.04\n",
      "Epoch 35  -  loss: 0.0287  time: 17.49\n",
      "---------------------------------------\n",
      "Train Accuracy: 99.50%\n",
      "Test Accuracy: 99.47%\n",
      "---------------------------------------\n",
      "Epoch 36  -  loss: 0.028  time: 17.0\n",
      "Epoch 37  -  loss: 0.0271  time: 17.0\n",
      "Epoch 38  -  loss: 0.0279  time: 16.99\n",
      "Epoch 39  -  loss: 0.0282  time: 16.98\n",
      "Epoch 40  -  loss: 0.0279  time: 16.97\n",
      "---------------------------------------\n",
      "Train Accuracy: 99.49%\n",
      "Test Accuracy: 99.48%\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(network, criterion=criterion, optimizer=optimizer, scheduler=scheduler, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
